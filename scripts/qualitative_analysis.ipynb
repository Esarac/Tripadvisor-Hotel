{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUALITATIVE ANALYSIS\n",
    "## TRIPADVISOR: HOTEL\n",
    "* Esteban Ariza\n",
    "* Johan Giraldo\n",
    "* Mateo Valdes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREREQUISITES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install textblob\n",
    "# %pip install vanderSentiment\n",
    "# %pip install nrclex\n",
    "# %pip install pysentiment2\n",
    "# %pip install pandas\n",
    "# %pip install nltk\n",
    "# %pip install nrclex\n",
    "# %pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import pysentiment2 as ps\n",
    "\n",
    "from nrclex import NRCLex\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for NRCLex\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sustainable hotels and other hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.abspath('')\n",
    "SUSTAINABLE_HOTELS_FILE_PATH = '../data/exploratory_analysis/tripadvisor_hotels_sustainable_clean.csv'\n",
    "OTHER_HOTELS_FILE_PATH = '../data/exploratory_analysis/tripadvisor_hotels_nonsustainable_clean.csv'\n",
    "\n",
    "sustainable_csv_path = os.path.join(dir, SUSTAINABLE_HOTELS_FILE_PATH)\n",
    "print(sustainable_csv_path)\n",
    "\n",
    "others_csv_path = os.path.join(dir, OTHER_HOTELS_FILE_PATH)\n",
    "print(others_csv_path)\n",
    "\n",
    "sust_df = pd.read_csv(sustainable_csv_path)\n",
    "others_df = pd.read_csv(others_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df['REVIEW_DATE'] = sust_df['REVIEW_DATE'].str.replace('Date of stay:', '')\n",
    "others_df['REVIEW_DATE'] = others_df['REVIEW_DATE'].str.replace('Date of stay:', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the \"TRIMESTER\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df.REVIEW_DATE = pd.to_datetime(sust_df.REVIEW_DATE)\n",
    "sust_df['TRIMESTER'] = pd.PeriodIndex(sust_df.REVIEW_DATE, freq=\"Q\").strftime('%Y-Q%q')\n",
    "sust_df['TRIMESTER'].head()\n",
    "\n",
    "others_df.REVIEW_DATE = pd.to_datetime(others_df.REVIEW_DATE)\n",
    "others_df['TRIMESTER'] = pd.PeriodIndex(others_df.REVIEW_DATE, freq=\"Q\").strftime('%Y-Q%q')\n",
    "others_df['TRIMESTER'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_counter(dataframe, n):\n",
    "    dict = {}\n",
    "    for tuple in dataframe.itertuples():\n",
    "        # text = tuple['REVIEW_TEXT']\n",
    "        text = tuple.REVIEW_TEXT\n",
    "        # trimester = tuple['trimester']\n",
    "        trimester = tuple.TRIMESTER\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "        tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
    "        ngram = ngrams(tokens, n)\n",
    "\n",
    "        for item in ngram:\n",
    "            word = \" \".join(item)\n",
    "\n",
    "            if trimester not in dict:\n",
    "                dict[trimester] = Counter()    \n",
    "            \n",
    "            dict[trimester][word] += 1\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_unigram_counter = get_ngram_counter(sust_df, 1)\n",
    "sust_bigram_counter = get_ngram_counter(sust_df, 2)\n",
    "sust_trigram_counter = get_ngram_counter(sust_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_unigram_counter = get_ngram_counter(others_df, 1)\n",
    "others_bigram_counter = get_ngram_counter(others_df, 2)\n",
    "others_trigram_counter = get_ngram_counter(others_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(others_unigram_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sust_unigram_counter.json\", \"w\") as out:\n",
    "    json.dump(sust_unigram_counter, out)\n",
    "\n",
    "with open(\"sust_bigram_counter.json\", \"w\") as out:\n",
    "    json.dump(sust_bigram_counter, out)\n",
    "\n",
    "with open(\"sust_trigram_counter.json\", \"w\") as out:\n",
    "    json.dump(sust_trigram_counter, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"others_unigram_counter.json\", \"w\") as out:\n",
    "    json.dump(others_unigram_counter, out)\n",
    "\n",
    "with open(\"others_bigram_counter.json\", \"w\") as out:\n",
    "    json.dump(others_bigram_counter, out)\n",
    "\n",
    "with open(\"others_trigram_counter.json\", \"w\") as out:\n",
    "    json.dump(others_trigram_counter, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sust_unigram_counter.items():\n",
    "    print(key)\n",
    "    print(value.most_common(20))\n",
    "\n",
    "for key, value in sust_bigram_counter.items():\n",
    "    print(key)\n",
    "    print(value.most_common(20))\n",
    "\n",
    "for key, value in sust_trigram_counter.items():\n",
    "    print(key)\n",
    "    print(value.most_common(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in others_unigram_counter.items():\n",
    "    print(key)\n",
    "    print(value.most_common(20))\n",
    "\n",
    "for key, value in others_bigram_counter.items():\n",
    "    print(key)\n",
    "    print(value.most_common(20))\n",
    "\n",
    "for key, value in others_trigram_counter.items():\n",
    "    print(key)\n",
    "    print(value.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionaries\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "hiv4 = ps.HIV4()\n",
    "lm = ps.LM()\n",
    "\n",
    "# Create aux methods\n",
    "def getPolarity(row):\n",
    "    analysis = TextBlob(row)\n",
    "    # print([analysis.sentiment.polarity, analysis.sentiment.subjectivity])\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "def getSubjectivity(row):\n",
    "    analysis = TextBlob(row)\n",
    "    # print([analysis.sentiment.polarity, analysis.sentiment.subjectivity])\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Positive': 4,\n",
       " 'Negative': 1,\n",
       " 'Polarity': 0.599999880000024,\n",
       " 'Subjectivity': 0.4545454132231443}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiv4.get_score(hiv4.tokenize('Im super happy to announce that im becoming a great father, because im actually a horrible father'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get polarity and subjetivity from the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df['TB_polarity'] = sust_df['REVIEW_TEXT'].apply(lambda row : getPolarity(row))\n",
    "sust_df['TB_subjectivity'] = sust_df['REVIEW_TEXT'].apply(lambda row: getSubjectivity(row))\n",
    "\n",
    "others_df['TB_polarity'] = others_df['REVIEW_TEXT'].apply(lambda row : getPolarity(row))\n",
    "others_df['TB_subjectivity'] = others_df['REVIEW_TEXT'].apply(lambda row: getSubjectivity(row))\n",
    "\n",
    "\n",
    "others_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain the scores (vader, HIV4, LM, NRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df['vader_score'] = sust_df['REVIEW_TEXT'].apply(lambda row : vader_analyzer.polarity_scores(row))\n",
    "others_df['vader_score'] = others_df['REVIEW_TEXT'].apply(lambda row : vader_analyzer.polarity_scores(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df['HIV4_score'] = sust_df['REVIEW_TEXT'].apply(lambda row: hiv4.get_score(hiv4.tokenize(row)))\n",
    "others_df['HIV4_score'] = others_df['REVIEW_TEXT'].apply(lambda row: hiv4.get_score(hiv4.tokenize(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df['LM_score'] = sust_df['REVIEW_TEXT'].apply(lambda row: lm.get_score(lm.tokenize(row)))\n",
    "others_df['LM_score'] = others_df['REVIEW_TEXT'].apply(lambda row: lm.get_score(lm.tokenize(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df['NRC_affect'] = sust_df['REVIEW_TEXT'].apply(lambda row: NRCLex(row).affect_frequencies)\n",
    "others_df['NRC_affect'] = others_df['REVIEW_TEXT'].apply(lambda row: NRCLex(row).affect_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df = pd.concat([sust_df, sust_df['vader_score'].apply(pd.Series)], axis=1)\n",
    "sust_df = pd.concat([sust_df, sust_df['LM_score'].apply(pd.Series).add_prefix('LM_')], axis=1)\n",
    "sust_df = pd.concat([sust_df, sust_df['HIV4_score'].apply(pd.Series).add_prefix('HIV4_')], axis=1)\n",
    "\n",
    "sust_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_df = pd.concat([others_df, others_df['vader_score'].apply(pd.Series)], axis=1)\n",
    "others_df = pd.concat([others_df, others_df['LM_score'].apply(pd.Series).add_prefix('LM_')], axis=1)\n",
    "others_df = pd.concat([others_df, others_df['HIV4_score'].apply(pd.Series).add_prefix('HIV4_')], axis=1)\n",
    "\n",
    "others_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sust_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save df into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUST_OUTPUT_PATH = '../data/qualitative_analysis/sentiment/tripadvisor_hotels_sustainable_sentiments.csv'\n",
    "OTHER_OUTPUT_PATH = '../data/qualitative_analysis/sentiment/tripadvisor_hotels_nonsustainable_sentiments.csv'\n",
    "\n",
    "sust_df.to_csv(SUST_OUTPUT_PATH, index=False)\n",
    "others_df.to_csv(OTHER_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_FILE_PATH = '../data/review_summarizer/summarized_reviews_by_year_and_hotel-small.csv'\n",
    "summary_df = pd.read_csv(SUMMARY_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(from_range, to_range, value):\n",
    "    on_decimal = to_range[0] + ( (value - from_range[0]) * (to_range[1] - to_range[0]) / (from_range[1] - from_range[0]) )\n",
    "    return round(on_decimal*2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vaderRating(text):\n",
    "    compound = vader_analyzer.polarity_scores(text)[\"compound\"]\n",
    "    return normalize((-1, 1), (1, 5), compound)\n",
    "\n",
    "summary_df['VADER_RATING'] = summary_df['REVIEW_SUMMARY'].apply(vaderRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df['VADER_RATING'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HIV4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiv4 = ps.HIV4()\n",
    "\n",
    "def hiv4Rating(text):\n",
    "    polarity = hiv4.get_score(hiv4.tokenize(text))['Polarity']\n",
    "    return normalize((-1, 1), (1, 5), polarity)\n",
    "\n",
    "summary_df['HIV4_RATING'] = summary_df['REVIEW_SUMMARY'].apply(hiv4Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df['HIV4_RATING'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ps.LM()\n",
    "\n",
    "def lmRating(text):\n",
    "    polarity = lm.get_score(lm.tokenize(text))['Polarity']\n",
    "    return normalize((-1, 1), (1, 5), polarity)\n",
    "\n",
    "summary_df['LM_RATING'] = summary_df['REVIEW_SUMMARY'].apply(lmRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df['LM_RATING'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overallRating(row):\n",
    "    return round( statistics.mean([row['VADER_RATING'], row['HIV4_RATING'], row['LM_RATING']])*2 )/2\n",
    "\n",
    "summary_df['OVERALL_RATING'] = summary_df.apply(overallRating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df['OVERALL_RATING'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries with rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of reviews by rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n1 bubble:', summary_df[summary_df['OVERALL_RATING'] == 1].iloc[0]['REVIEW_SUMMARY'])\n",
    "print('\\n2 bubbles:', summary_df[summary_df['OVERALL_RATING'] == 2].iloc[0]['REVIEW_SUMMARY'])\n",
    "print('\\n3 bubbles:', summary_df[summary_df['OVERALL_RATING'] == 3].iloc[0]['REVIEW_SUMMARY'])\n",
    "print('\\n4 bubbles:', summary_df[summary_df['OVERALL_RATING'] == 4].iloc[0]['REVIEW_SUMMARY'])\n",
    "print('\\n5 bubbles:', summary_df[summary_df['OVERALL_RATING'] == 5].iloc[0]['REVIEW_SUMMARY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save df into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_OUTPUT_PATH = '../data/qualitative_analysis/sentiment/tripadvisor_hotels_summaries_sentiments-small.csv'\n",
    "summary_df.to_csv(SUMMARY_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add the real hotel rating to the \"summary_df\" we need to import first the csv that we use to do the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGINAL_SUST_HOTELS_FILE_PATH = '../data/exploratory_analysis/tripadvisor_hotels_sustainable_clean.csv'\n",
    "ORIGINAL_OTHER_HOTELS_FILE_PATH = '../data/exploratory_analysis/tripadvisor_hotels_nonsustainable_clean.csv'\n",
    "\n",
    "original_sust_df = pd.read_csv(ORGINAL_SUST_HOTELS_FILE_PATH)\n",
    "original_other_df = pd.read_csv(ORIGINAL_OTHER_HOTELS_FILE_PATH)\n",
    "\n",
    "original_df = pd.concat([original_sust_df, original_other_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add the actual hotel rating column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_df = original_df.groupby([\"HOTEL_NAME\", \"HOTEL_RATING\",\"HOTEL_LOCATION\"]).size().reset_index(name='HOTEL_REVIEW_COUNT').drop(['HOTEL_REVIEW_COUNT'], axis=1)\n",
    "\n",
    "join_df = summary_df.join(hotel_df.set_index('HOTEL_NAME'), on='HOTEL_NAME', validate='m:1')\n",
    "join_df['HOTEL_RATING'] = join_df['HOTEL_RATING'].map(lambda x: x / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "ratings_types = [\n",
    "    np.full(shape=join_df.shape[0], fill_value=\"OVERALL\").tolist(),\n",
    "    np.full(shape=join_df.shape[0], fill_value=\"REAL\").tolist(),\n",
    "    np.full(shape=join_df.shape[0], fill_value=\"VADER\").tolist(),\n",
    "    np.full(shape=join_df.shape[0], fill_value=\"HIV4\").tolist(),\n",
    "    np.full(shape=join_df.shape[0], fill_value=\"LM\").tolist(),\n",
    "]\n",
    "\n",
    "rating_values = [\n",
    "    join_df[\"OVERALL_RATING\"].tolist(),\n",
    "    join_df[\"HOTEL_RATING\"].tolist(),\n",
    "    join_df[\"VADER_RATING\"].tolist(),\n",
    "    join_df[\"HIV4_RATING\"].tolist(),\n",
    "    join_df[\"LM_RATING\"].tolist(),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "ratings_df = pd.DataFrame(data={\n",
    "    \"TYPE\":flatten(ratings_types),\n",
    "    \"RATING\":flatten(rating_values)\n",
    "})\n",
    "\n",
    "ratings_df\n",
    "\n",
    "sns.kdeplot(\n",
    "    data=ratings_df,\n",
    "    x=\"RATING\",\n",
    "    hue=\"TYPE\",\n",
    "    # bw_adjust=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.loc[:, join_df.columns!='REVIEW_DATE'].describe()\n",
    "# join_df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save \"join_df\" into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHR_OUTPUT_PATH = '../data/qualitative_analysis/sentiment/tripadvisor_hotels_summaries_whr_sentiments-small.csv'\n",
    "join_df.to_csv(WHR_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import datasets [Disclaimer: Get the path to the actual file]\n",
    "df_3s = pd.read_csv(\"../data/Iter2Lda/tripadvisor_hotels_3_clean.csv\") # 3 Stars Hotels\n",
    "df_5sy = pd.read_csv(\"../data/Iter2Lda/tripadvisor_hotels_sustainable_clean_5stars.csv\") # 5 Stars Sustainable Hotels\n",
    "df_5sn = pd.read_csv(\"../data/Iter2Lda/tripadvisor_hotels_nonsustainable_clean_5stars.csv\") # 5 Stars Non-Sustainable Hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 1 (5 Stars Sustainable Hotels + 3 Stars Hotels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs\n",
    "df_d1 = pd.concat([df_3s, df_5sy])\n",
    "# Polarity\n",
    "df_d1['TB_polarity'] = df_d1['REVIEW_TEXT'].apply(lambda row : getPolarity(row))\n",
    "# Subjectivity\n",
    "df_d1['TB_subjectivity'] = df_d1['REVIEW_TEXT'].apply(lambda row: getSubjectivity(row))\n",
    "# VADER\n",
    "df_d1['vader_score'] = df_d1['REVIEW_TEXT'].apply(lambda row : vader_analyzer.polarity_scores(row))\n",
    "# HIV4\n",
    "df_d1['HIV4_score'] = df_d1['REVIEW_TEXT'].apply(lambda row: hiv4.get_score(hiv4.tokenize(row)))\n",
    "# LM\n",
    "df_d1['LM_score'] = df_d1['REVIEW_TEXT'].apply(lambda row: lm.get_score(lm.tokenize(row)))\n",
    "# NRC\n",
    "df_d1['NRC_affect'] = df_d1['REVIEW_TEXT'].apply(lambda row: NRCLex(row).affect_frequencies)\n",
    "\n",
    "# Format dictionaries results\n",
    "df_d1 = pd.concat([\n",
    "    df_d1\n",
    "    ,df_d1['vader_score'].apply(pd.Series).add_prefix('VADER_')\n",
    "    ,df_d1['HIV4_score'].apply(pd.Series).add_prefix('HIV4_')\n",
    "    ,df_d1['LM_score'].apply(pd.Series).add_prefix('LM_')\n",
    "    ,df_d1['NRC_affect'].apply(pd.Series).add_prefix('NRC_')\n",
    "], axis=1)\n",
    "\n",
    "# Drop unnecesary columns\n",
    "df_d1 = df_d1.drop(columns=['vader_score','HIV4_score','LM_score','NRC_affect'])\n",
    "\n",
    "df_d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "df_d1.to_csv(\"../data/sentiment/sentiment_d1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "df_d1_l = pd.read_csv(\"../data/sentiment/sentiment_d1.csv\")\n",
    "df_d1_l.describe()\n",
    "df_d1_l.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year column\n",
    "df_d1_l['REVIEW_YEAR'] = df_d1_l['REVIEW_DATE'].apply(lambda x: int(str(x).split('-')[0]))\n",
    "\n",
    "# Group by hotel and year\n",
    "df_d1_lg = df_d1_l[\n",
    "    [\n",
    "        'HOTEL_NAME','REVIEW_YEAR',\n",
    "        'TB_polarity', 'TB_subjectivity',\n",
    "        # 'VADER_neg', 'VADER_neu', 'VADER_pos',\n",
    "        'VADER_compound',\n",
    "        # 'HIV4_Positive', 'HIV4_Negative',\n",
    "        'HIV4_Polarity','HIV4_Subjectivity',\n",
    "        # 'LM_Positive', 'LM_Negative',\n",
    "        'LM_Polarity', 'LM_Subjectivity',\n",
    "        'NRC_fear', 'NRC_anger', 'NRC_anticip', 'NRC_trust','NRC_surprise', 'NRC_positive', 'NRC_negative', 'NRC_sadness','NRC_disgust', 'NRC_joy', 'NRC_anticipation'\n",
    "    ]\n",
    "].groupby(['HOTEL_NAME','REVIEW_YEAR']).mean()\n",
    "\n",
    "# Save data\n",
    "df_d1_lg.to_csv(\"../data/sentiment/sentiment_hy_d1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2 (5 Stars Non Sustainable Hotels + 3 Stars Hotels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs\n",
    "df_d2 = pd.concat([df_3s, df_5sn])\n",
    "\n",
    "# Polarity\n",
    "df_d2['TB_polarity'] = df_d2['REVIEW_TEXT'].apply(lambda row : getPolarity(row))\n",
    "# Subjectivity\n",
    "df_d2['TB_subjectivity'] = df_d2['REVIEW_TEXT'].apply(lambda row: getSubjectivity(row))\n",
    "# VADER\n",
    "df_d2['vader_score'] = df_d2['REVIEW_TEXT'].apply(lambda row : vader_analyzer.polarity_scores(row))\n",
    "# HIV4\n",
    "df_d2['HIV4_score'] = df_d2['REVIEW_TEXT'].apply(lambda row: hiv4.get_score(hiv4.tokenize(row)))\n",
    "# LM\n",
    "df_d2['LM_score'] = df_d2['REVIEW_TEXT'].apply(lambda row: lm.get_score(lm.tokenize(row)))\n",
    "# NRC\n",
    "df_d2['NRC_affect'] = df_d2['REVIEW_TEXT'].apply(lambda row: NRCLex(row).affect_frequencies)\n",
    "\n",
    "# Format dictionaries results\n",
    "df_d2 = pd.concat([\n",
    "    df_d2\n",
    "    ,df_d2['vader_score'].apply(pd.Series).add_prefix('VADER_')\n",
    "    ,df_d2['HIV4_score'].apply(pd.Series).add_prefix('HIV4_')\n",
    "    ,df_d2['LM_score'].apply(pd.Series).add_prefix('LM_')\n",
    "    ,df_d2['NRC_affect'].apply(pd.Series).add_prefix('NRC_')\n",
    "], axis=1)\n",
    "\n",
    "# Drop unnecesary columns\n",
    "df_d2 = df_d2.drop(columns=['vader_score','HIV4_score','LM_score','NRC_affect'])\n",
    "\n",
    "df_d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "df_d2.to_csv(\"../data/sentiment/sentiment_d2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariza\\AppData\\Local\\Temp\\ipykernel_24988\\1664337518.py:2: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_d2_l = pd.read_csv(\"../data/sentiment/sentiment_d2.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOTEL_RATING</th>\n",
       "      <th>HOTEL_PRICE</th>\n",
       "      <th>REVIEW_RATING</th>\n",
       "      <th>REVIEW_HELPFUL_VOTES</th>\n",
       "      <th>TB_polarity</th>\n",
       "      <th>TB_subjectivity</th>\n",
       "      <th>VADER_neg</th>\n",
       "      <th>VADER_neu</th>\n",
       "      <th>VADER_pos</th>\n",
       "      <th>VADER_compound</th>\n",
       "      <th>...</th>\n",
       "      <th>NRC_anger</th>\n",
       "      <th>NRC_anticip</th>\n",
       "      <th>NRC_trust</th>\n",
       "      <th>NRC_surprise</th>\n",
       "      <th>NRC_positive</th>\n",
       "      <th>NRC_negative</th>\n",
       "      <th>NRC_sadness</th>\n",
       "      <th>NRC_disgust</th>\n",
       "      <th>NRC_joy</th>\n",
       "      <th>NRC_anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.0</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>136667.000000</td>\n",
       "      <td>121532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>45.373133</td>\n",
       "      <td>219.153475</td>\n",
       "      <td>4.548004</td>\n",
       "      <td>1.298002</td>\n",
       "      <td>0.332626</td>\n",
       "      <td>0.332626</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>0.742179</td>\n",
       "      <td>0.235862</td>\n",
       "      <td>0.861698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178173</td>\n",
       "      <td>0.060543</td>\n",
       "      <td>0.332822</td>\n",
       "      <td>0.059390</td>\n",
       "      <td>0.034864</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.170320</td>\n",
       "      <td>0.131198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.590084</td>\n",
       "      <td>287.566380</td>\n",
       "      <td>0.831729</td>\n",
       "      <td>1.760918</td>\n",
       "      <td>0.162276</td>\n",
       "      <td>0.162276</td>\n",
       "      <td>0.032937</td>\n",
       "      <td>0.095590</td>\n",
       "      <td>0.104159</td>\n",
       "      <td>0.322031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072250</td>\n",
       "      <td>0.052853</td>\n",
       "      <td>0.110458</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.026467</td>\n",
       "      <td>0.068797</td>\n",
       "      <td>0.061436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.995700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.233886</td>\n",
       "      <td>0.233886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333431</td>\n",
       "      <td>0.333431</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.963300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.981700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>3001.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HOTEL_RATING    HOTEL_PRICE  REVIEW_RATING  REVIEW_HELPFUL_VOTES  \\\n",
       "count  136667.000000  136667.000000  136667.000000         136667.000000   \n",
       "mean       45.373133     219.153475       4.548004              1.298002   \n",
       "std         3.590084     287.566380       0.831729              1.760918   \n",
       "min        30.000000       0.000000       1.000000              0.000000   \n",
       "25%        45.000000      76.000000       4.000000              1.000000   \n",
       "50%        45.000000     140.000000       5.000000              1.000000   \n",
       "75%        50.000000     252.000000       5.000000              1.000000   \n",
       "max        50.000000    3001.000000       5.000000            209.000000   \n",
       "\n",
       "         TB_polarity  TB_subjectivity      VADER_neg      VADER_neu  \\\n",
       "count  136667.000000    136667.000000  136667.000000  136667.000000   \n",
       "mean        0.332626         0.332626       0.021958       0.742179   \n",
       "std         0.162276         0.162276       0.032937       0.095590   \n",
       "min        -1.000000        -1.000000       0.000000       0.144000   \n",
       "25%         0.233886         0.233886       0.000000       0.682000   \n",
       "50%         0.333431         0.333431       0.008000       0.750000   \n",
       "75%         0.434259         0.434259       0.034000       0.811000   \n",
       "max         1.000000         1.000000       0.441000       1.000000   \n",
       "\n",
       "           VADER_pos  VADER_compound  ...      NRC_anger  NRC_anticip  \\\n",
       "count  136667.000000   136667.000000  ...  136667.000000     136667.0   \n",
       "mean        0.235862        0.861698  ...       0.018988          0.0   \n",
       "std         0.104159        0.322031  ...       0.035038          0.0   \n",
       "min         0.000000       -0.995700  ...       0.000000          0.0   \n",
       "25%         0.162000        0.911800  ...       0.000000          0.0   \n",
       "50%         0.229000        0.963300  ...       0.000000          0.0   \n",
       "75%         0.302000        0.981700  ...       0.033333          0.0   \n",
       "max         0.856000        0.999700  ...       1.000000          0.0   \n",
       "\n",
       "           NRC_trust   NRC_surprise   NRC_positive   NRC_negative  \\\n",
       "count  136667.000000  136667.000000  136667.000000  136667.000000   \n",
       "mean        0.178173       0.060543       0.332822       0.059390   \n",
       "std         0.072250       0.052853       0.110458       0.073977   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.137931       0.000000       0.268293       0.000000   \n",
       "50%         0.181818       0.058824       0.322581       0.043478   \n",
       "75%         0.222222       0.093750       0.384615       0.090909   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "         NRC_sadness    NRC_disgust        NRC_joy  NRC_anticipation  \n",
       "count  136667.000000  136667.000000  136667.000000     121532.000000  \n",
       "mean        0.034864       0.010431       0.170320          0.131198  \n",
       "std         0.046416       0.026467       0.068797          0.061436  \n",
       "min         0.000000       0.000000       0.000000          0.016393  \n",
       "25%         0.000000       0.000000       0.133333          0.090909  \n",
       "50%         0.000000       0.000000       0.175000          0.125000  \n",
       "75%         0.058824       0.000000       0.214286          0.160000  \n",
       "max         1.000000       0.500000       1.000000          1.000000  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysis\n",
    "df_d2_l = pd.read_csv(\"../data/sentiment/sentiment_d2.csv\")\n",
    "df_d2_l.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year column\n",
    "df_d2_l['REVIEW_YEAR'] = df_d2_l['REVIEW_DATE'].apply(lambda x: int(str(x).split('-')[0]))\n",
    "\n",
    "# Group by hotel and year\n",
    "df_d2_lg = df_d2_l[\n",
    "    [\n",
    "        'HOTEL_NAME','REVIEW_YEAR',\n",
    "        'TB_polarity', 'TB_subjectivity',\n",
    "        # 'VADER_neg', 'VADER_neu', 'VADER_pos',\n",
    "        'VADER_compound',\n",
    "        # 'HIV4_Positive', 'HIV4_Negative',\n",
    "        'HIV4_Polarity','HIV4_Subjectivity',\n",
    "        # 'LM_Positive', 'LM_Negative',\n",
    "        'LM_Polarity', 'LM_Subjectivity',\n",
    "        'NRC_fear', 'NRC_anger', 'NRC_anticip', 'NRC_trust','NRC_surprise', 'NRC_positive', 'NRC_negative', 'NRC_sadness','NRC_disgust', 'NRC_joy', 'NRC_anticipation'\n",
    "    ]\n",
    "].groupby(['HOTEL_NAME','REVIEW_YEAR']).mean()\n",
    "\n",
    "# Save data\n",
    "df_d2_lg.to_csv(\"../data/sentiment/sentiment_hy_d2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3 (3 Stars Hotels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs\n",
    "df_d3 = df_3s\n",
    "\n",
    "# Polarity\n",
    "df_d3['TB_polarity'] = df_d3['REVIEW_TEXT'].apply(lambda row : getPolarity(row))\n",
    "# Subjectivity\n",
    "df_d3['TB_subjectivity'] = df_d3['REVIEW_TEXT'].apply(lambda row: getSubjectivity(row))\n",
    "# VADER\n",
    "df_d3['vader_score'] = df_d3['REVIEW_TEXT'].apply(lambda row : vader_analyzer.polarity_scores(row))\n",
    "# HIV4\n",
    "df_d3['HIV4_score'] = df_d3['REVIEW_TEXT'].apply(lambda row: hiv4.get_score(hiv4.tokenize(row)))\n",
    "# LM\n",
    "df_d3['LM_score'] = df_d3['REVIEW_TEXT'].apply(lambda row: lm.get_score(lm.tokenize(row)))\n",
    "# NRC\n",
    "df_d3['NRC_affect'] = df_d3['REVIEW_TEXT'].apply(lambda row: NRCLex(row).affect_frequencies)\n",
    "\n",
    "# Format dictionaries results\n",
    "df_d3 = pd.concat([\n",
    "    df_d3\n",
    "    ,df_d3['vader_score'].apply(pd.Series).add_prefix('VADER_')\n",
    "    ,df_d3['HIV4_score'].apply(pd.Series).add_prefix('HIV4_')\n",
    "    ,df_d3['LM_score'].apply(pd.Series).add_prefix('LM_')\n",
    "    ,df_d3['NRC_affect'].apply(pd.Series).add_prefix('NRC_')\n",
    "], axis=1)\n",
    "\n",
    "# Drop unnecesary columns\n",
    "df_d3 = df_d3.drop(columns=['vader_score','HIV4_score','LM_score','NRC_affect'])\n",
    "\n",
    "df_d3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "df_d3.to_csv(\"../data/sentiment/sentiment_d3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOTEL_RATING</th>\n",
       "      <th>HOTEL_PRICE</th>\n",
       "      <th>REVIEW_RATING</th>\n",
       "      <th>REVIEW_HELPFUL_VOTES</th>\n",
       "      <th>TB_polarity</th>\n",
       "      <th>TB_subjectivity</th>\n",
       "      <th>VADER_neg</th>\n",
       "      <th>VADER_neu</th>\n",
       "      <th>VADER_pos</th>\n",
       "      <th>VADER_compound</th>\n",
       "      <th>...</th>\n",
       "      <th>NRC_anger</th>\n",
       "      <th>NRC_anticip</th>\n",
       "      <th>NRC_trust</th>\n",
       "      <th>NRC_surprise</th>\n",
       "      <th>NRC_positive</th>\n",
       "      <th>NRC_negative</th>\n",
       "      <th>NRC_sadness</th>\n",
       "      <th>NRC_disgust</th>\n",
       "      <th>NRC_joy</th>\n",
       "      <th>NRC_anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.0</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>68468.000000</td>\n",
       "      <td>61022.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>44.643483</td>\n",
       "      <td>93.512955</td>\n",
       "      <td>4.469197</td>\n",
       "      <td>1.182552</td>\n",
       "      <td>0.315818</td>\n",
       "      <td>0.315818</td>\n",
       "      <td>0.022451</td>\n",
       "      <td>0.744209</td>\n",
       "      <td>0.233338</td>\n",
       "      <td>0.860392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182975</td>\n",
       "      <td>0.059481</td>\n",
       "      <td>0.327046</td>\n",
       "      <td>0.061598</td>\n",
       "      <td>0.036102</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>0.168529</td>\n",
       "      <td>0.131067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.927921</td>\n",
       "      <td>90.284917</td>\n",
       "      <td>0.840206</td>\n",
       "      <td>1.538144</td>\n",
       "      <td>0.154645</td>\n",
       "      <td>0.154645</td>\n",
       "      <td>0.032851</td>\n",
       "      <td>0.095521</td>\n",
       "      <td>0.103767</td>\n",
       "      <td>0.320219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070610</td>\n",
       "      <td>0.052227</td>\n",
       "      <td>0.108826</td>\n",
       "      <td>0.076739</td>\n",
       "      <td>0.047461</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.066396</td>\n",
       "      <td>0.060830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220949</td>\n",
       "      <td>0.220949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.909200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316536</td>\n",
       "      <td>0.316536</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HOTEL_RATING   HOTEL_PRICE  REVIEW_RATING  REVIEW_HELPFUL_VOTES  \\\n",
       "count  68468.000000  68468.000000   68468.000000          68468.000000   \n",
       "mean      44.643483     93.512955       4.469197              1.182552   \n",
       "std        3.927921     90.284917       0.840206              1.538144   \n",
       "min       30.000000      0.000000       1.000000              0.000000   \n",
       "25%       45.000000      0.000000       4.000000              1.000000   \n",
       "50%       45.000000     94.000000       5.000000              1.000000   \n",
       "75%       45.000000    135.000000       5.000000              1.000000   \n",
       "max       50.000000    432.000000       5.000000             67.000000   \n",
       "\n",
       "        TB_polarity  TB_subjectivity     VADER_neg     VADER_neu  \\\n",
       "count  68468.000000     68468.000000  68468.000000  68468.000000   \n",
       "mean       0.315818         0.315818      0.022451      0.744209   \n",
       "std        0.154645         0.154645      0.032851      0.095521   \n",
       "min       -1.000000        -1.000000      0.000000      0.238000   \n",
       "25%        0.220949         0.220949      0.000000      0.685000   \n",
       "50%        0.316536         0.316536      0.010000      0.752000   \n",
       "75%        0.412500         0.412500      0.034000      0.813000   \n",
       "max        1.000000         1.000000      0.427000      1.000000   \n",
       "\n",
       "          VADER_pos  VADER_compound  ...     NRC_anger  NRC_anticip  \\\n",
       "count  68468.000000    68468.000000  ...  68468.000000      68468.0   \n",
       "mean       0.233338        0.860392  ...      0.019693          0.0   \n",
       "std        0.103767        0.320219  ...      0.035836          0.0   \n",
       "min        0.000000       -0.989600  ...      0.000000          0.0   \n",
       "25%        0.159000        0.909200  ...      0.000000          0.0   \n",
       "50%        0.225000        0.961800  ...      0.000000          0.0   \n",
       "75%        0.299000        0.981100  ...      0.034483          0.0   \n",
       "max        0.762000        0.999600  ...      1.000000          0.0   \n",
       "\n",
       "          NRC_trust  NRC_surprise  NRC_positive  NRC_negative   NRC_sadness  \\\n",
       "count  68468.000000  68468.000000  68468.000000  68468.000000  68468.000000   \n",
       "mean       0.182975      0.059481      0.327046      0.061598      0.036102   \n",
       "std        0.070610      0.052227      0.108826      0.076739      0.047461   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.142857      0.000000      0.263158      0.000000      0.000000   \n",
       "50%        0.185185      0.058824      0.315789      0.045455      0.022222   \n",
       "75%        0.222222      0.090909      0.375000      0.093750      0.062500   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        NRC_disgust       NRC_joy  NRC_anticipation  \n",
       "count  68468.000000  68468.000000      61022.000000  \n",
       "mean       0.009907      0.168529          0.131067  \n",
       "std        0.026528      0.066396          0.060830  \n",
       "min        0.000000      0.000000          0.017857  \n",
       "25%        0.000000      0.133333          0.090909  \n",
       "50%        0.000000      0.173913          0.125000  \n",
       "75%        0.000000      0.210526          0.160000  \n",
       "max        0.500000      1.000000          1.000000  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysis\n",
    "df_d3_l = pd.read_csv(\"../data/sentiment/sentiment_d3.csv\")\n",
    "df_d3_l.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year column\n",
    "df_d3_l['REVIEW_YEAR'] = df_d3_l['REVIEW_DATE'].apply(lambda x: int(str(x).split('-')[0]))\n",
    "\n",
    "# Group by hotel and year\n",
    "df_d3_lg = df_d3_l[\n",
    "    [\n",
    "        'HOTEL_NAME','REVIEW_YEAR',\n",
    "        'TB_polarity', 'TB_subjectivity',\n",
    "        # 'VADER_neg', 'VADER_neu', 'VADER_pos',\n",
    "        'VADER_compound',\n",
    "        # 'HIV4_Positive', 'HIV4_Negative',\n",
    "        'HIV4_Polarity','HIV4_Subjectivity',\n",
    "        # 'LM_Positive', 'LM_Negative',\n",
    "        'LM_Polarity', 'LM_Subjectivity',\n",
    "        'NRC_fear', 'NRC_anger', 'NRC_anticip', 'NRC_trust','NRC_surprise', 'NRC_positive', 'NRC_negative', 'NRC_sadness','NRC_disgust', 'NRC_joy', 'NRC_anticipation'\n",
    "    ]\n",
    "].groupby(['HOTEL_NAME','REVIEW_YEAR']).mean()\n",
    "\n",
    "# Save data\n",
    "df_d3_lg.to_csv(\"../data/sentiment/sentiment_hy_d3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
